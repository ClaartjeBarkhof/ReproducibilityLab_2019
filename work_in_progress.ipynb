{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import random \n",
    "import gym\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from actor_critic import ActorCritic\n",
    "\n",
    "\n",
    "def run_episodes_policy_gradient(env, num_episodes, max_steps, discount_factor, learn_rate, nstep=1, n_hidden=256):\n",
    "    \n",
    "    n_state_features = len(env.reset())\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    model =  ActorCritic(n_state_features, n_actions, n_hidden)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "\n",
    "    # loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        # initialize S (first state of episode)\n",
    "        s = env.reset()\n",
    "        print('start state', s)\n",
    "        I = 1\n",
    "        step = 0\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        states = [s]\n",
    "        rewards = []\n",
    "        while True:\n",
    "            if t < T:\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    v_s, pi_s_a = model.forward(s)\n",
    "            \n",
    "                # select action\n",
    "                a = torch.multinomial(pi_s_a, 1).item()  \n",
    "                log_prob = torch.log(pi_s_a.squeeze(0)[a])\n",
    "            \n",
    "                # take action\n",
    "                s_new, r, done, _ = env.step(a)\n",
    "                rewards.append(r)\n",
    "                \n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    states.append(s_new)\n",
    "                    \n",
    "            tau = t - nstep + 1\n",
    "            \n",
    "            if tau >= 0:\n",
    "                print('tau larger than 0')\n",
    "                \n",
    "                G = np.sum(gamma**(i-tau) * rewards[i] for i in range(tau, min(tau+nstep, T)))\n",
    "                if tau + nstep < T:\n",
    "                    with torch.no_grad():\n",
    "                        # Look ahead one step (t+1)\n",
    "                        v_s_new, _ = model.forward(s_new)\n",
    "                        G += (gamma**nstep) + v_s_new\n",
    "\n",
    "                state_tau = states[tau]\n",
    "                v_tau, pi_s_a_tau = model.forward(state_tau)\n",
    "                log_prob_tau = torch.log(pi_s_a_tau.squeeze(0)[a])\n",
    "                \n",
    "                delta = G - v_tau\n",
    "                loss_a =  - delta * I * log_prob_tau\n",
    "                loss_c =  - delta * v_tau # should this be negative or positive?\n",
    "                loss = loss_a + loss_c\n",
    "                print('loss', loss)\n",
    "\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                print('t:', t, 'v_s_t:',v_s.item(), 'v_s_t+1:', v_s_new.item(), \n",
    "                      'tau:', tau, 'G:', G, 'delta:', delta, 'done:', done)\n",
    "            else:\n",
    "                print('t:', t)\n",
    "            # update episode and records\n",
    "            if t < T:  # we took an action above\n",
    "                s = s_new\n",
    "                \n",
    "            # episode step\n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "            \n",
    "        if episode % 10 == 0:\n",
    "            #print(\"E{0}- Steps:{1} Loss:{2}\".format(episode, step, loss))\n",
    "            pass\n",
    "                  \n",
    "        episode_durations.append(step)\n",
    "        \n",
    "    return episode_durations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop for each step of the episode $t=0,1, \\ldots, T-1:$\n",
    "$$\n",
    "\\begin{array}{l}{G \\leftarrow \\sum_{k=t+1}^{T} \\gamma^{k-t-1} R_{k}} \\\\ {\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\alpha \\gamma^{t} G \\nabla \\ln \\pi\\left(A_{t} | S_{t}, \\boldsymbol{\\theta}\\right)}\\end{array}\n",
    "$$\n",
    "* The REINFORCE loss is defined as $- \\sum_t \\log \\pi_\\theta(a_t|s_t) G_t$, which means that you should compute the (discounted) return $G_t$ for all $t$\n",
    "\n",
    "\n",
    "\n",
    "Here we have\n",
    "Take action $A,$ observe $S^{\\prime}, R$ \n",
    "$$\n",
    "\\delta \\leftarrow R+\\gamma \\hat{v}\\left(S^{\\prime}, \\mathbf{w}\\right)-\\hat{v}(S, \\mathbf{w}) \\\\\n",
    "\\mathbf{w} \\leftarrow \\mathbf{w}+\\alpha^{\\mathbf{w}} \\delta \\nabla \\hat{v}(S, \\mathbf{w}) \\\\ \\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta}+\\alpha^{\\boldsymbol{\\theta}} I \\delta \\nabla \\ln \\pi(A | S, \\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start state [ 0.00560942  0.01842265 -0.03590751 -0.0120678 ]\n",
      "t: 0\n",
      "t: 1\n",
      "tau larger than 0\n",
      "loss tensor([[2.3530]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claartje/miniconda3/lib/python3.6/site-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "/Users/claartje/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:61: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-e9c908f0e957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                                                                  \u001b[0mlearn_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                                                                  \u001b[0mnstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                                                                  n_hidden=256)\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode_durations_policy_gradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode durations per episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-48526c1c07f5>\u001b[0m in \u001b[0;36mrun_episodes_policy_gradient\u001b[0;34m(env, num_episodes, max_steps, discount_factor, learn_rate, nstep, n_hidden)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "def smooth(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "num_episodes = 400\n",
    "discount_factor = 0.99\n",
    "learn_rate = 0.001\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "env = gym.envs.make(\"CartPole-v0\")\n",
    "env.seed(seed)\n",
    "\n",
    "max_steps =  300 # max steps per episode\n",
    "\n",
    "episode_durations_policy_gradient = run_episodes_policy_gradient(env, \n",
    "                                                                 num_episodes, \n",
    "                                                                 max_steps,\n",
    "                                                                 discount_factor, \n",
    "                                                                 learn_rate,\n",
    "                                                                 nstep=3,\n",
    "                                                                 n_hidden=256)\n",
    "plt.plot(smooth(episode_durations_policy_gradient, 20))\n",
    "plt.title('Episode durations per episode')\n",
    "plt.legend(['Policy gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch \n",
    "import random \n",
    "import gym\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from actor_critic import ActorCritic\n",
    "\n",
    "\n",
    "def run_episodes_policy_gradient(env, num_episodes, max_steps, discount_factor, learn_rate, nstep=1, n_hidden=256):\n",
    "    \n",
    "    n_state_features = len(env.reset())\n",
    "    n_actions = env.action_space.n\n",
    "    \n",
    "    model =  ActorCritic(n_state_features, n_actions, n_hidden)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), learn_rate)\n",
    "    \n",
    "    episode_durations = []\n",
    "\n",
    "    # loop for each episode\n",
    "    for episode in range(num_episodes):\n",
    "        \n",
    "        # initialize S (first state of episode)\n",
    "        s = env.reset()\n",
    "        print('start state', s)\n",
    "        I = 1\n",
    "        step = 0\n",
    "        \n",
    "        T = float('inf')\n",
    "        t = 0\n",
    "        \n",
    "        states = [s]\n",
    "        rewards = []\n",
    "        while True:\n",
    "            print('t', t)\n",
    "            if t < T:\n",
    "            \n",
    "                with torch.no_grad():\n",
    "                    v_s, pi_s_a = model.forward(s)\n",
    "                    print('v_s', v_s)\n",
    "                    print('pi_s_a', pi_s_a)\n",
    "            \n",
    "                # select action\n",
    "                a = torch.multinomial(pi_s_a, 1).item()  \n",
    "                print('a', a)\n",
    "                log_prob = torch.log(pi_s_a.squeeze(0)[a])\n",
    "            \n",
    "                # take action\n",
    "                s_new, r, done, _ = env.step(a)\n",
    "                print('s_new', s_new)\n",
    "                print('r', r)\n",
    "                print('done', done)\n",
    "                rewards.append(r)\n",
    "                \n",
    "                if done:\n",
    "                    T = t + 1\n",
    "                else:\n",
    "                    states.append(s_new)\n",
    "                \n",
    "                        \n",
    "            # compute delta \n",
    "#             v_s_new = 0 \n",
    "#             with torch.no_grad():\n",
    "#                 if not done:\n",
    "#                     v_s_new, _ = model.forward(s_new)\n",
    "#                     v_s_new = v_s_new.item() \n",
    "#                 delta = r + (gamma * v_s_new) - v_s.item()\n",
    "            \n",
    "            tau = t - nstep + 1\n",
    "            print('tau', tau)\n",
    "            if tau >= 0:\n",
    "                G = np.sum(gamma**(i-tau) * rewards[i] for i in range(tau, min(tau+nstep, T)))\n",
    "                if tau + nstep < T:\n",
    "                    with torch.no_grad():\n",
    "                        # Look ahead one step (t+1)\n",
    "                        v_s_new, _ = model.forward(s_new)\n",
    "                        G += (gamma**nstep) + v_s_new\n",
    "                print('G', G)\n",
    "\n",
    "                state_tau = states[tau]\n",
    "                print('state_tau', state_tau)\n",
    "                v_tau, pi_s_a_tau = model.forward(state_tau)\n",
    "                print('v_tau', v_tau)\n",
    "                log_prob_tau = torch.log(pi_s_a_tau.squeeze(0)[a])\n",
    "                \n",
    "                delta = G - v_tau\n",
    "                print('delta', delta)\n",
    "\n",
    "                # from another github where the whole thing works:\n",
    "                # adv = r - v_s.item()\n",
    "                # loss_a = -log_prob * adv\n",
    "                # loss_c = torch.nn.functional.smooth_l1_loss(v_s, torch.Tensor([r]).float())\n",
    "\n",
    "                # compute gradient\n",
    "                # minus\n",
    "                loss_a =  - delta * I * log_prob_tau\n",
    "                loss_c =  - delta * v_tau # should this be negative or positive?\n",
    "                print('loss_a', loss_a)\n",
    "                print('loss_c', loss_c)\n",
    "                loss = loss_a + loss_c\n",
    "\n",
    "                # backprop\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # update episode and records\n",
    "            if t < T:  # we took an action above\n",
    "                s = s_new\n",
    "                \n",
    "            # episode step\n",
    "            t += 1\n",
    "            if tau == T - 1:\n",
    "                break\n",
    "\n",
    "#             I = discount_factor*I\n",
    "#             s = s_new\n",
    "#             step += 1\n",
    "\n",
    "            # until s is a terminal state or we used the max steps\n",
    "            if done or step > max_steps:\n",
    "                break\n",
    "            \n",
    "        if episode % 10 == 0:\n",
    "            #print(\"E{0}- Steps:{1} Loss:{2}\".format(episode, step, loss))\n",
    "            pass\n",
    "                  \n",
    "        episode_durations.append(step)\n",
    "        \n",
    "    return episode_durations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
